{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MI5XSqQ039ca",
        "Vt1LorWI3XSz",
        "698RB6XDTFY9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JUMAAAN/projectpractice/blob/master/%5B%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%EA%B8%B0%EC%B4%88%5D_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%84_%ED%86%B5%ED%95%9C_%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5_%EC%9D%B4%ED%95%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[인공지능 기초] 선형회귀분석을 통한 기계학습 이해\n",
        "\n",
        "작성: 이규호(정보, 세화고등학교)\n",
        "\n",
        "도움을 주신 선생님: 김지은(수학, 세화고등학교)\n",
        "\n",
        "참고 도서: 모두의 딥러닝, 코딩 강화 파이썬\n",
        "\n",
        "초안 작성: 2022년 09월 12일\n",
        "\n",
        "최종 수정: 2022년 09월 21일\n",
        "<br/><br/>\n",
        "\n",
        "## A. 이 문서를 읽고 이해하기 위해서\n",
        "\n",
        "1. 외부 패키지(keras, numpy, matplotlib 등)의 사용법을 알아야 할 필요는 없습니다. 이 문서에서는 사용하지 않습니다.\n",
        "2. 다항함수의 미분법을 알고 있어야 합니다.\n",
        "3. Python 기초 문법(입력, 처리, 출력, 변수, 자료형 등)을 알고 있어야 합니다. 본 문서에 Python 특유의 문법은 되도록 사용하지 않았으며, 코드 효율보다는 수식의 형태를 보존해서 코드로 옮기는 것을 우선했습니다.\n",
        "<br/><br/>\n",
        "\n",
        "## B. 인공지능(Artificial Engineering)이란?\n",
        "\n",
        "사람(**인**)이 만들어낸(**공**) **지**적 **능**력\n",
        "\n",
        "'인공'이라는 단어에는, 지금껏 지능이란 자연적으로 주어지는 것이었으나 **이제는 만들어낼 수 있는 것**이라는 맥락이 담겨 있습니다.\n",
        "<br/><br/>\n",
        "\n",
        "## C. 기계학습(Machine Learning)이란?\n",
        "\n",
        "**기계**가 배우고(**학**) 익히는(**습**) 것\n",
        "\n",
        "'기계'라는 표현은, 지금껏 학습은 인간의 전유물이었으나 **이제는 그렇지 않다**는 맥락을 담고 있습니다. 기계학습은 인공지능 제작 방법 중 하나이며, 알고리즘 설계 패러다임(Algorithm Design Paradigm) 중 하나로 보기도 합니다.\n",
        "\n",
        "※ 전문가 시스템: 기계학습을 이용하지 않고 규칙 기반 프로그래밍으로 만든 인공지능.\n",
        "\n",
        "※ 고등학교 정보 교과에서 다루는 알고리즘 설계 패러다임: Greedy, Brute Force, Dynamic Programming, Divide and Conquer\n",
        "<br/><br/>\n",
        "## D. 기계학습의 등장 후, 무엇이 달라졌을까?\n",
        "\n",
        "전통적인 알고리즘과 기계학습 알고리즘 사이에는 큰 패러다임 차이가 있습니다.\n",
        "\n",
        "함수 구조(Input, Process, Output)에 빗대어 설명하자면, 전통적인 알고리즘은 Process 그 자체입니다. 컴퓨터에게 Process를 자세히 지시하고 Input을 주면, 컴퓨터가 Input을 대상으로 정해진 Process를 수행해서 Output을 내놓았습니다.\n",
        "\n",
        "그러나 기계학습 알고리즘은 Process가 아닙니다. 기계학습 알고리즘은 컴퓨터가 (Input과 Output쌍으로 구성된)학습용 데이터를 통해 Process를 알아서 만들어낼 수 있도록 돕는 **방법**입니다. 직관적으로 설명하자면 Process를 만들어내기 위한 알고리즘인 셈입니다(Process는 Model 형태로 만들어지는데, 자세한 설명은 뒤로 미루겠습니다).\n",
        "\n",
        "전통적인 방식(기계학습이 아닌, 절차를 직접 제시하는 방식의 알고리즘)이 쓸모없어진 것은 결코 아닙니다. 여전히 많은 일들이 기계학습이 아닌 알고리즘에 따라 자동화되어 처리되고 있고, 앞으로도 그럴 것입니다. \n",
        "\n",
        "다만 전통적인 알고리즘으로는 자동화하기가 무척 어렵거나 불가능했던 작업들 중에서, 기계학습 알고리즘으로 자동화할 수 있게 된 것들이 많이 생겼습니다. 예컨대 **사람 목소리를 듣고 누구인지 알아맞히기, 사진을 보고 어떤 동물인지 분류하기, 같은 화풍으로 그림 그리기, 가상의 아나운서 목소리로 뉴스 진행하기 등**이 이에 해당합니다.\n",
        "\n",
        "자동화란 사람이 일을 수행하지 않고 컴퓨터에게 미루어 알아서 처리되도록 하는 것을 의미합니다. 그런데 컴퓨터는 지치지도 않고 불만도 없으며 어마어마하게 정확하고 빠르기 때문에, 컴퓨터의 생산성은 인간이 지닌 생산성을 아득하게 초월합니다. 결국 사람이 해야만 했던 일 중 상당수를 컴퓨터로 자동화할 수 있게 되면서, 이로 인해 산업 구조가 급격히 바뀌며 <a href=\"https://terms.naver.com/entry.naver?docId=778938&cid=42085&categoryId=42085\">마찰적 실업</a>이 대량으로 발생하고 있습니다. 새로운 일자리와 산업도 많이 만들어지고 있으므로 꼭 나쁜 일인 것만은 아니지만, 일시적인 사회 혼란은 불가피할 것으로 보입니다.\n",
        "<br/><br/>\n",
        "\n",
        "## E. 그렇다면 지금 우리는 무엇을 해야 할까?\n",
        "\n",
        "어떤 작업이 인공지능으로 자동화될 수 있는 것인지 또는 그렇지 않은 것인지 구분해낼 수 있는 판단력을 갖춰야 합니다. 그래야 기계로 대체될 수 있는 일에 소중한 시간과 노동력을 집중하는 불상사를 막고, 현명한 진로 선택을 할 수 있기 때문입니다. \n",
        "\n",
        "이제는 자동화 가능 여부를 판단하려면 기계학습 원리를 이해하고 있어야 합니다. 그리고 기계학습의 원리 이해를 위한 첫 단계는, 프로그래밍 언어를 이용해 선형회귀분석을 구현하는 것입니다."
      ],
      "metadata": {
        "id": "2twA9SxZ-VZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. 선형회귀분석(Linear Regression)이란?\n",
        "$xy$평면상에 존재하는 $n$개의 점$\\{(x_i,y_i)|i=1, 2, 3,..., n\\}$을 가장 잘 설명하는 직선($y=ax+b$)을 찾는 것입니다. 즉, 직선의 $a$(기울기)와 $b$($y$절편)를 알아내는 것이 선형회귀분석의 목적입니다.\n",
        "\n",
        "※ 고등학생을 위해서 $xy$평면으로 논의의 범위를 제한합니다.\n",
        "\n",
        "※ 회귀(Regression)라는 이름은 프랜시스 골턴(Francis Galton)의 논문 '평균으로의 회귀(Regression toward the mean)'에서 유래했다고 합니다. 자세한 내용은 <a href=\"https://brunch.co.kr/@plusstar/139\">검색해보세요</a>.\n",
        "<br/><br/>\n",
        "\n",
        "## G. 선형회귀분석은 왜 할까?\n",
        "$x$와 $y$사이의 관계식은 일종의 패턴입니다. 이 패턴이 계속됨을 전제할 수 있다면, 회귀식을 이용해 발생하지 않은 상황(독립변수: $x$)에 대한 **예측**(종속변수: $y$)이 가능합니다. 예컨대 기온과 아이스크림 판매량 사이의 관계식을 미리 알아둔다면, 아이스크림 제조사와 유통사는 기상청의 예보를 이용해 미래에 필요할 재고의 적정량을 **예측**하고 불필요한 비용(재료비, 보관비 등)을 낮출 수 있을 것입니다.\n",
        "\n",
        "※ 함수가 주어지면 $x$에 의해 $y$값이 결정되므로, $y$값은 $x$에 종속됩니다. 이런 이유로 (실험이나 관찰 상황에서) $y$를 종속변수라 합니다. 반면 x값은 그렇지 않으므로 독립변수라 합니다.\n",
        "\n",
        "※ 사물을 이해하면 미래를 예측할 수 있고, 미래를 예측하면 상황을 통제할 수 있으며, 상황을 통제하면 불확실성을 없앨 수 있습니다. 그리고 불확실성을 없애고 싶은 욕구는 인간의 본능입니다. 그래서 사람들은 관심을 가진 대상을 이해하고 예측하려 노력합니다. 이에 관한 자세한 내용은 행동경제학의 <a href=\"https://ko.wikipedia.org/wiki/전망_이론\">전망 이론</a> 등 관련 정보를 참고하세요.\n",
        "<br/><br/>\n",
        "\n",
        "## H. 수학이 아닌 **정보(인공지능 기초)** 수업 시간에, 느닷없이 선형회귀분석을 왜 공부해야 할까?\n",
        "\n",
        "인공지능을 깊이 있게 이해하기 위해서는, 기계학습을 이용한 **회귀·분류·군집화** 등을 프로그래밍 언어로 직접 구현해보는 과정이 필수적입니다. 교과서에서도 **지도학습**의 예시로 **예측(회귀)**와 **분류**를, 그리고 **비지도학습**의 예시로 **군집화**를 제시하고 있습니다. 이중에서 회귀를 가장 먼저 다루는 데에는 여러 가지 이유가 있습니다.\n",
        "1. 가장 직관적으로 이해할 수 있는 주제입니다.\n",
        "2. 딥러닝(인공신경망을 이용한 기계학습)을 이해하기 위해 필수적인 내용입니다.\n",
        "3. 선형회귀분석을 위한 최소제곱법 접근 중 두 가지 방법(연립방정식 풀이와 **경사하강법**)을 대조함으로써 알고리즘 설계 패러다임 변화를 실감할 수 있습니다. 연립방정식 풀이를 통해 유도한 공식을 따르는 것은 전통적인 알고리즘, 경사하강법은 기계학습 알고리즘에 해당하기 때문입니다.\n",
        "<br/><br/>\n",
        "\n",
        "## I. '선형회귀분석의 이해와 코드 구현'이 그렇게 좋고 중요한데 왜 고등학교 교과서에 없을까?\n",
        "\n",
        "단적으로 말하자면 어려워서 그렇습니다. 이 내용을 이해하려면 미분도 알아야 하고 프로그래밍 언어 1개를 자유롭게 사용할 수 있어야 합니다. 게다가 편미분과 시그마 기호($\\Sigma$)를 포함한 수식의 미분 등 다뤄야 하는 수학도 고등학교 교육과정을 아주 살짝 넘어갑니다.\n",
        "\n",
        "게다가 대학교에서 배울 내용을 고등학교 시험에 출제하게 되면, 그 형태와 경우에 따라서 공교육정상화촉진및선행교육규제에관한특별법(선행학습금지법)에 저촉될 수 있습니다. 무엇보다 내용 자체가 수학과 정보 그 사이 어딘가에 있어서, 수학 선생님이 프로그래밍을 하거나 정보 선생님이 수학을 해야 가르칠 수 있는 내용입니다. 그래서 일선의 모든 고등학교에서 다루기에는 다소 무리가 있고, 이 때문에 교과서에서도 빠져 있습니다.\n",
        "\n",
        "벌써부터 지레 겁먹을 필요는 없습니다. 이 문서에서 다루는 수학은 **목적**이 아닙니다. 인공지능을 이해하고 구현하기 위한 **수단**입니다. 새로운 개념 이해가 필요한 부분은 없고, 몇몇 공식이나 규칙을 '그렇다더라' 하고 받아들여 사용할 수 있는 수준이면 충분합니다. 극한값을 구하는 문제에 (학교 수학 선생님은 증명할 수 없으면 쓰지 말라 하시는)<a href=\"https://namu.wiki/w/%EB%A1%9C%ED%94%BC%ED%83%88%EC%9D%98%20%EC%A0%95%EB%A6%AC#s-4\">로피탈의 정리</a>를 사용하는 것과 같은 느낌입니다.\n",
        "\n",
        "부디 행렬과 벡터가 고등학교 수학 공통 교육과정에 돌아오기를, 더 나아가 선형회귀분석이 고등학교 교육과정에 포함되기를, 정보 교사로서 간절히 기원합니다."
      ],
      "metadata": {
        "id": "MI5XSqQ039ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## J. 표기법과 공식 등에 관한 안내\n",
        "\n",
        "선형회귀분석에 대한 본격적인 이야기를 시작하기 전에, 미리 알아두어야 할 수학 공식과 성질, 약속 등이 있어 안내합니다. 단, **고등학교 수학 정규 교과목에서의 설명과 다른 표기나 설명을 사용하는 부분이 여럿 있으므로 헷갈리지 않도록 주의하길 바랍니다.**\n",
        "\n",
        "1. 먼저 편미분입니다. 대학교에 가면 고등학교 교육과정에서는 다루지 않는 다변수함수, 예컨대 $y=f(x_1, x_2)$같은 것을 미분하는 방법을 배우게 됩니다. 편미분이란 미분하고자 하는 대상만 변수로 보고, 나머지 변수를 상수 취급해서 미분하는 것입니다. 예를 들어 \n",
        "###<center>$y=3x_1+2x_1x_2+7x_2+8$</center>\n",
        "의 $x_1$에 대한 편미분은 다음과 같습니다. <a href=\"https://ko.wikipedia.org/wiki/∂\">$∂$은 d와 같이 '디'나 '라운드 디' 또는 '파셜 디' 등으로 읽습니다.</a>\n",
        "###<center>$\\frac{\\partial y}{\\partial x_1}=3+2x_2$</center>\n",
        "$x_2$에 대한 편미분은 다음과 같습니다.\n",
        "\n",
        "###<center>$\\frac{\\partial y}{\\partial x_2}=2x_1+7$</center>\n",
        "<br/>\n",
        "\n",
        "2. 다음으로는 시그마(Σ)를 포함한 식의 미분입니다. 고등학교 수학 교육과정에 포함되어 있지 않습니다. **유한 개의 항**을 더하는 상황에서, 다음이 성립합니다.\n",
        "\n",
        "###<center>$\\{\\sum f(x)\\}'=\\sum f'(x)$</center>\n",
        "<center>$f'(x)$는 $f(x)$를 미분하여 얻는 도함수입니다.</center>\n",
        "<br/>\n",
        "\n",
        "3. 다음은 기댓값 표기입니다. 다음과 같이 여러 $x$가 주어진 확률적 상황에서 $x$의 기댓값은 $x$의 (산술)평균과 같은 말입니다. $X = x_1, x_2, x_3, ..., x_n$의 기댓값은 $E(X)$로 표기하며, 다음과 같이 구할 수 있습니다.\n",
        "###<center>$E(X) = \\frac{1}{n}\\sum\\limits_{i=1}^nx_i$</center>\n",
        "※ 확률과 통계 시간에는 $x_1, x_2, x_3, ..., x_n$의 확률이 $p_1,p_2,p_3, ..., p_n$일 때 $E(X)=\\sum\\limits_{i=1}^nx_ip_i$로 정의하는데, 이 문서에서는 각 $p_i$가 $\\frac{1}{n}$로 동일하므로 위와 같이 표기했습니다. 이하 분산 및 공분산 설명에서도 동일하게 적용했습니다.\n",
        "<br/>\n",
        "\n",
        "4. 다음은 분산(Variance)입니다. 분산은 중학생 때 배운 후 확률과 통계 과목에서 다시 등장합니다. 기억하지 못하는 고등학생들을 위해서 다시 설명하자면, '데이터가 평균으로부터 얼마나 떨어져 있는지 한 눈에 알아보기 위한 지표'입니다. 이를 위해 각 데이터의 편차(=변량-평균)를 제곱하여 평균을 구하면 분산을 얻을 수 있습니다. 이때 **편차**가 아닌 **편차의 제곱**을 대상으로 평균을 구하는 이유는, 음수인 편차와 양수의 편차를 단순히 더할 경우 부호에 의한 상쇄효과가 발생하기 때문입니다. <br/>즉 $Var(X)$ 또는 $σ_x^2$ 등으로 표기하는 $X = x_1, x_2, x_3, ..., x_n$의 분산은, 다음과 같이 구할 수 있습니다.\n",
        "###<center>$Var(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n\\{x_i-E(X)\\}^2\\\\\n",
        "=\\frac{1}{n}\\sum\\limits_{i=1}^n\\{x_i^2-2x_iE(X)+E(X)^2\\}\\\\\n",
        "=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i^2-2E(X)̇̇⋅\\frac{1}{n}\\sum\\limits_{i=1}^nx_i+E(X)^2\\\\\n",
        "=E(X^2)-2E(X)⋅E(X)+E(X)^2\\\\\n",
        "=E(X^2)-E(X)^2$</center>\n",
        "<center>고등학교 확률과 통계 과목에서는 $Var(X$)가 아닌 $V(X)$로 표기합니다.</center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "5. 다음은 표준편차(Standard Deviation)입니다. $X=x_1,x_2,x_3,...,x_n$의 표준편차는 $σ_X$로 표기하며 다음과 같이 구할 수 있습니다.<br/>\n",
        "###<center>$σ_X = \\sqrt{Var(X)}$</center>\n",
        "분산을 구할 때 편차를 제곱해서 사용했으므로, 크기를 다시 줄이기 위해(Scale Down) 양의 제곱근을 구하는 것이라 생각하면 됩니다.\n",
        "<br/>\n",
        "\n",
        "6. 다음은 공분산(Covariance)입니다. 고등학교 과정에서는 다루지 않습니다. 공분산은 상관계수를 구하는 과정에서 주로 계산하며, 대개 $X$와 $Y$의 단위가 다르므로 의미 있는 값이 산출되기는 쉽지 않습니다.\n",
        "<br/>분산의 정의를 알고 있다면 공분산의 정의를 받아들이기는 쉽습니다. 분산이란 편차 제곱들의 평균, 즉 '$x_i$의 편차와 $x_i$의 편차의 곱'들을 대상으로 평균을 계산한 것이며, 공분산은 $x_i$의 편차와 $y_i$의 편차의 곱'들을 대상으로 평균을 구한 것입니다. \n",
        "<br/>즉 $Cov(X,Y) 또는 σ_{XY}$로 표기하는 $X = x_1, x_2, x_3, ..., x_n$과 $Y = y_1, y_2, y_3, ..., y_n$의 공분산은 다음과 같이 구할 수 있습니다.\n",
        "\n",
        "###<center>$Cov(X,Y) = \\frac{1}{n}\\sum\\limits_{i=1}^n\\{x_i-E(X)\\}\\{y_i-E(Y)\\}\\\\ \n",
        "=\\frac{1}{n}\\sum\\limits_{i=1}^n\\{x_iy_i-x_iE(Y)-y_iE(X)+E(X)E(Y)\\}\\\\\n",
        "=\\frac{1}{n}\\sum\\limits_{i=1}^nx_iy_i-E(Y)⋅\\frac{1}{n}\\sum\\limits_{i=1}^nx_i-E(X)⋅\\frac{1}{n}\\sum\\limits_{i=1}^ny_i+E(X)E(Y)\\\\\n",
        "=E(XY)-E(Y)E(X)-E(X)E(Y)+E(X)E(Y)\\\\\n",
        "=E(XY)-E(X)E(Y)$</center>\n",
        "<br/>\n",
        "\n",
        "7. 다음은 상관계수(Correlation Coefficient)입니다. 공분산과 마찬가지로 고등학교 수학 교육과정에서는 다루지 않으나, 인공지능 기초 교과서에서는 해석 방법에 대한 설명이 일부 등장합니다. 상관계수는 $X$와 $Y$의 상관관계를 확인하기 위해 산출합니다. 상관계수는 Corr(X,Y) 또는 $\\rho$ 등으로 표기하며, 다음과 같이 산출할 수 있습니다.\n",
        "###<center>$Corr(X,Y)=\\rho = \\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y}$</center>\n",
        "상관계수는 산출 방법 뿐만 아니라 해석도 중요합니다. 상관계수는 1 이하의 절댓값을 가지는데($-1\\leρ\\le1)$. 상관계수가 양수면 양의 상관관계($x$가 증가할 때 $y$도 증가), 음수면 음의 상관관계($x$가 증가할 때 $y$는 감소)에 있는 것입니다. 또한 절댓값이 0이면 관계가 없음을 의미하며, 1이면 완전한 선형 관계($y=ax+b$ 꼴)임을 의미합니다. 그러므로 상관계수를 산출했을 때 어느 정도 관계가 있다(대개 절댓값이 0.7 이상이면 강한 상관관계, 0.3 이상이면 약한 상관관계로 해석)는 사실이 확인되어야 선형회귀분석에 의미가 있을 것입니다.\n",
        "<br/>**상관계수가 0에 가깝다고 데이터에 패턴이 없는 것은 아닙니다.** 일례로, 원점 근처에 점이 집중적으로 찍혀 있을 경우에도 상관계수는 0에 가깝게 산출됩니다. 따라서 데이터의 패턴을 확인할 때에는 다양한 방법으로 시각화해서 확인해볼 필요가 있습니다.\n",
        "<br/>**또한, 강한 상관관계가 있더라도 그것이 인과관계를 의미하지는 않습니다.** 예컨대 아이스크림 판매량과 익사자의 수는 양의 선형관계를 보이는데, 기온이 올라가면 아이스크림도 많이 사먹고 물놀이도 많이 가기 때문입니다. 그런데 이것을 아이스크림 판매로 인해 익사자가 늘어났다고 판단하면 중대한 오류가 됩니다.\n",
        "<br/><br/>\n",
        "다음은 평균, 분산, 표준편차, 공분산을 구현하는 코드입니다."
      ],
      "metadata": {
        "id": "zgLfx8hJvXUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음 X와 Y에 대해 평균, 분산, 표준편차, 공분산을 구하는 코드를 작성\n",
        "X = [0, 2, 4, 6]\n",
        "Y = [1, 3, 2, 4]\n",
        "n = len(X)\n",
        "\n",
        "# 기댓값(평균)을 반환하는 함수\n",
        "# lambda 키워드를 이용하면 한 줄로 표현 가능\n",
        "def E(alist):\n",
        "  return sum(alist)/len(alist)\n",
        "\n",
        "# 편차: 리스트의 멤버 함수인 append나 복합할당연산자인 += 등을 이용하면 더 짧게 표현 가능\n",
        "deviation_X = []\n",
        "deviation_Y = []\n",
        "for i in range(n):\n",
        "  deviation_X = deviation_X + [X[i]-E(X)]\n",
        "  deviation_Y = deviation_Y + [Y[i]-E(Y)]\n",
        "\n",
        "# 분산: 편차 제곱의 평균\n",
        "deviation_X_square = []\n",
        "deviation_Y_square = []\n",
        "for i in range(n):\n",
        "  deviation_X_square = deviation_X_square + [deviation_X[i]**2]\n",
        "  deviation_Y_square = deviation_Y_square + [deviation_Y[i]**2]\n",
        "Var_X = E(deviation_X_square)\n",
        "Var_Y = E(deviation_Y_square)\n",
        "\n",
        "# 표준편차: 분산의 양의 제곱근\n",
        "sd_X = Var_X**0.5\n",
        "sd_Y = Var_Y**0.5\n",
        "\n",
        "# 공분산: 편차간 곱의 평균\n",
        "deviation_XY = []\n",
        "for i in range(n):\n",
        "  deviation_XY = deviation_XY + [(X[i]-E(X))*(Y[i]-E(Y))]\n",
        "Cov_XY = E(deviation_XY)\n",
        "\n",
        "# 상관계수\n",
        "Corr_XY = Cov_XY/(sd_X*sd_Y)\n",
        "\n",
        "print(f'X의 편차: {deviation_X}')\n",
        "print(f'Y의 편차: {deviation_Y}')\n",
        "print(f'X의 분산: {Var_X}')\n",
        "print(f'Y의 분산: {Var_Y}')\n",
        "print(f'X의 표준편차: {sd_X}')\n",
        "print(f'Y의 표준편차: {sd_Y}')\n",
        "print(f'X와 Y의 공분산: {Cov_XY}')\n",
        "print(f'X와 Y의 상관계수: {Corr_XY}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2NGVtzsd2A_",
        "outputId": "16080f24-f0dd-4402-a2c2-29fc1a3ed530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X의 편차: [-3.0, -1.0, 1.0, 3.0]\n",
            "Y의 편차: [-1.5, 0.5, -0.5, 1.5]\n",
            "X의 분산: 5.0\n",
            "Y의 분산: 1.25\n",
            "X의 표준편차: 2.23606797749979\n",
            "Y의 표준편차: 1.118033988749895\n",
            "X와 Y의 공분산: 2.0\n",
            "X와 Y의 상관계수: 0.7999999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 다음은 **경사하강법(Gradient Descent)**입니다. 경사하강법이란 기울기가 더 낮은 점으로 이동하기를 반복해 극소점을 구해내는 방법입니다. <br/>예를 들어 $f(x)=x^2-4x+1$에 대해 생각해보겠습니다. 우리는 이 함수를 미분하여 $f'(x)=2x-4$가 0이 되는 지점을 찾음(방정식 풀이)으로써 극소점이 $x=2$에 있다는 사실을 쉽게 알아낼 수 있지만, 경사하강법을 통해서도 알아낼 수 있습니다. 구체적으로 말하자면 임의의 $x_1$을 설정하고, 다음 점화식을 이용해 값을 충분히 많이 갱신하면 극소점의 $x$좌표에 근사할 수 있습니다.\n",
        "###<center>$x_{t+1}=x_{t}-f'(t)\\times lr$</center>\n",
        "<center>$lr$은 학습률(Learning Rate)의 머리글자입니다.</center><br/>\n",
        "위 식을 이해하기 위해, 우선은 맨 뒤에 곱한 $lr$이 없다고 생각해주세요. 그렇다면 위 식은, 현재의 $x$ 좌표인 $x_t$를 기준으로 다음 $x$좌표인 $x_{t+1}$를 향해 나아가려면 현재 지점에서의 접선 기울기($f'(x_{t})$)를 빼야한다고 설명하는 셈이 됩니다.\n",
        "<br/>이것은 이동 방향 측면에서 옳은 선택입니다. 왜냐하면, 접선의 기울기가 양수일 때는 극소점이 $x$축을 기준으로 음의 방향에 있고, 접선의 기울기가 음수일 때는 극소점이 $x$축을 기준으로 양의 방향에 있기 때문입니다. 한편, 이동 거리 측면에서도 옳은 선택입니다. 극소점에서 현재 위치까지의 $x$축 거리가 가까워질수록 접선의 기울기가 지니는 절댓값의 크기도 점차 작아지기 때문입니다.\n",
        "<br/>하지만 기울기를 그냥 빼게 되면 값이 너무 큽니다. 그리고 과하게 큰 값을 자꾸 빼면 극소점으로부터의 $x$축 거리가 가까워지면서 수렴하는 것이 아니라, 오히려 멀어지면서 발산하게 됩니다. 이런 문제를 해결하기 위해 0과 1사이의 값(대개 0.01, 0.03, 0.0001 등의 값)을 지니는 학습률(Learning Rate)을 기울기에 곱합니다. 학습률이 과하게 크면 곱하는 보람이 없이 발산하고, 너무 작으면 학습에 오랜 시간이 걸리므로 적당한 학습률을 사용해야 합니다. <br/>하지만 적당한 학습률을 설정하는 문제는 <a href=\"https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=gradient+descent+learning+rate&btnG=\">또 다른 하나의 주제이므로</a> 여기서 다루지 않겠습니다. 고등학생이라면, 발산하지 않는 수준에서 임의의 값을 사용하는 정도로 충분합니다.\n",
        "<br/>다음은 위에서 예시로 사용했던 $f(x)=x^2-4x+1$의 극소점의 $x$좌표를 경사하강법으로 찾는 코드입니다."
      ],
      "metadata": {
        "id": "GQJjNsH-dUVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복 횟수\n",
        "epochs = 10000\n",
        "\n",
        "# 학습률\n",
        "lr = 0.05\n",
        "\n",
        "# 도함수의 코드 표현\n",
        "def fprime(x):\n",
        "  return 2*x-4\n",
        "\n",
        "# 임의의 점에서 출발\n",
        "x_t = 2**50\n",
        "\n",
        "# x값 갱신\n",
        "for i in range(epochs):\n",
        "  x_t = x_t - fprime(x_t)*lr\n",
        "\n",
        "# 결과 출력\n",
        "print(f'극소점에서의 x좌표:{x_t}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgRcem_oa1EJ",
        "outputId": "5bec3535-0f6b-463e-c045-fc35e39f948b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "극소점에서의 x좌표:2.0000000000000018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K. 선형회귀분석(1): 문제 해석\n",
        "\n",
        "다짜고짜 결론부터 말하겠습니다. $xy$평면상에 존재하는 n개의 점 $\\{(x_i,y_i)| i= 1, 2, ..., n\\}$을 가장 잘 나타내는 직선($\\hat{y} = ax+b$)을 구하려면 MSE를 다음과 같이 정의하고, 이를 최소화하는 $a$와 $b$를 찾으면 됩니다.\n",
        "\n",
        "###<center>$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat{y_i})^2$</center>\n",
        "<center>$\\hat{y}$는 $y$햇이라고 읽습니다. 예측치를 실제치와 구분 표기하기 위한 것입니다.</center>\n",
        "<br/>\n",
        "\n",
        "그런데 MSE는 대체 무엇이며, 난데없이 MSE를 왜 위와 같이 정의할까요? 그리고 MSE를 최소화하는 $a$와 $b$가 $n$개의 점을 가장 잘 나타내는 직선의 기울기와 $y$절편이라니, 왜 그런 걸까요?\n",
        "\n",
        "우선은 오차를 $y_i$(실제치)에서 $\\hat{y_i}$를 뺀 값(=실제치와 예측치 사이의 $y$축 거리)으로 정의하는 이유부터 생각해보겠습니다. 왜 '점과 직선 사이의 (최단)거리'나 '$x$축 거리'가 아니라 $y$축 거리를 기준으로 오차를 측정할까요?\n",
        "\n",
        "그것은 우리가 관행적으로 $y$축에 종속변수를 놓기 때문입니다. 우리가 선형회귀분석을 하는 이유는, 아직은 발생하지 않은 상황(독립변수, $x$)에 따라 결정될 미래(종속변수, $y$)를 예측하기 위해서입니다. 예를 들어 기온과 아이스크림 판매량의 선형회귀분석 결과 $y=2x+3$을 얻었다고 가정해보겠습니다. 그런데 만약 기온이 10도인 날에 예측판매량인 23개를 넘어 25개가 판매되었다면, 우리는 자연스럽게 **2개**만큼의 오차가 발생했다고 생각할 것입니다. 이 **2개**가 바로 $y$축 거리로 구한 오차입니다.\n",
        "\n",
        "어떻게 하면 오차를 최소화할 수 있을까요? 미래에나 알 수 있는 관측값의 오차를 줄일 수는 없으므로, 지금까지 주어진 $n$개의 점과 선 사이에 존재하는 오차를 최소화하는 것이 우리의 최선입니다. 따라서 다음 수치를 최소화하면 될 것 같은 생각이 듭니다.\n",
        "\n",
        "###<center>$$오차총합 = \\sum\\limits_{i=1}^n(y_i-\\hat{y_i})$$</center>\n",
        "\n",
        "하지만 위 식에는 문제가 있습니다. 선보다 위쪽에 존재하는 점과 선 사이의 오차는 양수(+)로, 선보다 아래쪽에 존재하는 점과 선 사이의 오차는 음수(-)로 측정되기 때문에 이것들을 그냥 더하면 오차 간 상쇄가 발생합니다.\n",
        "\n",
        "이 문제를 해결하려면 오차의 부호를 없애야 합니다. 부호를 없앤다고 하면 절댓값이 먼저 떠오르지만, 절댓값을 사용하면 추후 여러 가지로 불편해집니다. 따라서 오차를 제곱해서 사용하겠습니다.\n",
        "\n",
        "###<center>$$오차제곱의총합 = \\sum\\limits_{i=1}^n(y_i-\\hat{y_i})^2$$</center>\n",
        "\n",
        "이 상태로 논의를 계속 진행해도 괜찮습니다. 하지만 점이 10개든 1억 개든 큰 단위 변동 없이 오차의 정도를 나타낼 수 있도록 총합이 아닌 평균을 구해두면, 추후 오차의 정도를 가늠하거나 비교하기 위한 지표로도 사용할 수 있지 않을까요?\n",
        "\n",
        "###<center>$$MSE(오차제곱의평균) = \\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat{y_i})^2$$</center>\n",
        "\n",
        "영어로는 오차(Error) 제곱의(Squared) 평균(Error)을 MSE(Mean Squared Error)라고 표기하는데, 간혹 이를 한글로 표기할 때 **평균제곱오차**라고 하는 경우가 있으므로 주의할 필요가 있습니다.\n",
        "\n",
        "지금까지 설명이 길었지만, 다음과 같이 짧게 요약할 수 있습니다.\n",
        "\n",
        ">**$xy$평면상에 존재하는 $n$개의 점을 가장 잘 나타내는 직선을 구하기 위해, MSE를 최소화하는 $a$와 $b$를 찾는다.**\n",
        "\n",
        "이러한 접근법을 **최소제곱법(Least Squares Method)**이라 합니다."
      ],
      "metadata": {
        "id": "Vt1LorWI3XSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L. 선형회귀분석(2): 문제 풀이법 생각해보기\n",
        "\n",
        "$\\hat{y} = ax+b$이므로, $x_i$에서의 $y$ 예측치는 $ax_i+b$입니다. 따라서 다음과 같이 나타낼 수 있습니다.\n",
        "\n",
        "###<center>$$MSE = \\frac{1}{n}∑\\{y_i-(ax_i+b)\\}^2$$</center>\n",
        "\n",
        "이때, (모든 $x_i$가 0인 극단적 상황만 아니라면) MSE는 $a$와 $b$ 각각에 대해 최고차항의 계수가 양수인 이차함수로 표현될 수 있습니다. 따라서 $a$와 $b$가 다음 두 식을 만족할 때 MSE는 최소가 됩니다.<br/><br/>\n",
        "\n",
        "###<center>①$\\space\\frac{∂MSE}{∂a}=0$</center><br/><center>②$\\space\\frac{∂MSE}{∂b}=0$</center><br>\n",
        "\n",
        "\n",
        "구하고자 하는 값이 $a$와 $b$ 두 개이며 식도 두 개인 상황입니다. 따라서 우리는 **연립방정식 풀이**를 통해 $a$와 $b$를 구해낼 수 있습니다. 또는, **경사하강법**으로 위 두 식을 만족하는 $a$와 $b$를 구할 수도 있을 것입니다. 아래에서 둘 모두 제시하겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "698RB6XDTFY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M. 선형회귀분석(3): 연립방정식 풀이를 통한 공식 유도 및 코드 작성\n",
        "###②$\\space\\frac{∂MSE}{∂b}\\\\\n",
        "=\\frac{2}{n}\\sum\\limits_{i=1}^n[\\{y_i-(ax_i+b)\\}(-1)]\\\\\n",
        "=2[-\\frac{1}{n}\\sum\\limits_{i=1}^n y_i+a\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^nx_i+b]\\\\\n",
        "=2[-E(Y)+aE(X)+b]=0\\space이므로$<br/><br/>\n",
        "###$\\space\\space\\space b=E(Y)-aE(X)$<br/><br/>\n",
        "###①$\\space\\frac{∂MSE}{∂a}\\\\\n",
        "=\\frac{2}{n}\\sum\\limits_{i=1}^n[\\{y_i-(ax_i+b)\\}(-x_i)]\\\\\n",
        "=2[-\\frac{1}{n}\\sum\\limits_{i=1}^n x_iy_i+a\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^nx_i^2+b\\cdot\\frac{1}{n}\\sum\\limits_{i=1}^n x_i]\\\\\n",
        "=2[-E(XY)+aE(X^2)+bE(X)]\\\\\n",
        "=2[-E(XY)+aE(X^2)+\\{E(Y)-aE(X)\\}E(X)]\\\\\n",
        "=2[-E(XY)+aE(X^2)+E(X)E(Y)-a\\{E(X)\\}^2]\\\\\n",
        "=2[a\\{E(X^2)-E(X)^2\\}-\\{E(XY)-E(X)E(Y)\\}]=0\\space이므로$<br/><br/>\n",
        "###$\\space\\space\\space a=\\frac{E(XY)-E(X)E(Y)}{E(X^2)-E(X)^2}=\\frac{Cov(X,Y)}{Var(X)}$"
      ],
      "metadata": {
        "id": "m0Oxh5vYnjvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 네 개의 점 (0,1), (2,3), (4,2), (6,4)에 대한 회귀식을 구하는 상황이라 가정하고 코드를 작성\n",
        "X = [0, 2, 4, 6]\n",
        "Y = [1, 3, 2, 4]\n",
        "\n",
        "# 기댓값(평균)을 반환하는 함수\n",
        "def E(alist):\n",
        "  return sum(alist)/len(alist)\n",
        "\n",
        "# 두 리스트에서 대응하는 요소들을 곱해 만든 새로운 리스트를 반환하는 함수\n",
        "# 내장함수인 zip 등을 사용하면 훨씬 단순하게 표현 가능\n",
        "def t(alist, blist):\n",
        "  tlist = []\n",
        "  for i in range(len(alist)):\n",
        "    tlist = tlist + [alist[i]*blist[i]]\n",
        "  return tlist\n",
        "\n",
        "XX = t(X, X)\n",
        "XY = t(X, Y)\n",
        "\n",
        "a = (E(XY)-E(X)*E(Y))/(E(XX)-E(X)*E(X))\n",
        "b = E(Y) - a*E(X)\n",
        "\n",
        "print(f'회귀식: y = {a}x + {b}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChD0ZV80q_Cv",
        "outputId": "200fce0d-70f2-4246-b6bd-1d42ec06dd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "회귀식: y = 0.4x + 1.2999999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N. 선형회귀분석(4): 경사하강법을 이용한 접근과 코드 작성\n",
        "임의의 $a_1$과 $b_1$를 설정한 뒤, 다음 점화식을 이용해 값을 연속적으로 갱신하면 됩니다.\n",
        "<br/>표기의 편의상 $\\space\\frac{∂MSE}{∂a}$를 $f(a,b)$로, $\\space\\frac{∂MSE}{∂b}$를 $g(a,b)$로 나타내겠습니다. $lr$은 학습률입니다.\n",
        "\n",
        "$\\space\\space\\space\\space\\space a_{t+1} = a_t-f(at,bt)*lr$<br/>\n",
        "$\\space\\space\\space\\space\\space b_{t+1} = b_t-g(at,bt)*lr$<br/>\n",
        "\n",
        "MSE의 $a$와 $b$에 대한 편미분은 각각 다음과 같음을 이미 확인했으므로, 그대로 코드에 옮겨 적용하겠습니다.\n",
        "\n",
        "###$\\space\\space\\space\\space\\space\\space\\frac{∂MSE}{∂a}=\\frac{2}{n}\\sum\\limits_{i=1}^n[\\{y_i-(ax_i+b)\\}(-x_i)]$<br/>\n",
        "###$\\space\\space\\space\\space\\space\\space\\frac{∂MSE}{∂b}=\\frac{2}{n}\\sum\\limits_{i=1}^n[\\{y_i-(ax_i+b)\\}(-1)]$<br/>"
      ],
      "metadata": {
        "id": "wmMlExCvq6eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 네 개의 점 (0,1), (2,3), (4,2), (6,4)에 대한 회귀식을 구하는 상황이라 가정하고 코드를 작성\n",
        "X = [0, 2, 4, 6]\n",
        "Y = [1, 3, 2, 4]\n",
        "n = len(X)\n",
        "\n",
        "# 반복 횟수\n",
        "epochs = 10000\n",
        "\n",
        "# 학습률\n",
        "lr = 0.05\n",
        "\n",
        "# 도함수의 코드 표현\n",
        "def dMSE_da(a, b):\n",
        "  s = 0\n",
        "  for i in range(n):\n",
        "    s = s + ((Y[i]-(a*X[i]+b))*(-X[i]))\n",
        "  return (2/n)*s\n",
        "\n",
        "def dMSE_db(a, b):\n",
        "  s = 0\n",
        "  for i in range(n):\n",
        "    s = s + ((Y[i]-(a*X[i]+b))*(-1))\n",
        "  return (2/n)*s\n",
        "\n",
        "# 임의의 점에서 출발\n",
        "a_t = 2**50\n",
        "b_t = 2**50\n",
        "\n",
        "# a, b값 갱신\n",
        "for i in range(epochs):\n",
        "  a_t = a_t - dMSE_da(a_t, b_t)*lr\n",
        "  b_t = b_t - dMSE_db(a_t, b_t)*lr\n",
        "\n",
        "# 결과 출력\n",
        "print(f'회귀식: y = {a_t}x + {b_t}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGoyn75coqmp",
        "outputId": "57459cf8-1f6e-4695-fdd6-c5f372b1d6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "회귀식: y = 0.3999999999999994x + 1.300000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##O. 마무리하며\n",
        "\n",
        "선형회귀분석을 수행하기 위해, 전통적인 알고리즘에 해당하는 최소제곱법의 연립방정식 풀이와 기계 학습 알고리즘에 해당하는 경사하강법을 비교하여 살펴보았습니다. \n",
        "\n",
        "수학만 잔뜩 공부한 것 같고, 인공지능은 구경도 못한 것 같은 느낌이 들 수도 있겠지만 실제로는 그렇지 않습니다. 선형회귀식($\\hat{y}=ax+b$)이라는 기계 학습 **모델(Model)**을 완성하기 위해, 학습 데이터($n$개의 점 $\\{(x_i,y_i)|i=1,2,...,n\\}$)를 **기계 학습 알고리즘(경사하강법)**에 적용해서 두 개의 **파라미터($a$, $b$)**를 최적화한 것입니다.\n",
        "\n",
        "#성공적인 첫 발을 내딛은 것을 축하합니다!\n",
        "<br/><br/>\n",
        "\n",
        "##P. 이어서 공부할 것들\n",
        "1. 주어진 $n$개의 점에 대해 오차를 완벽히 최소화하는 것은 오히려 실제 예측 때 오차를 높일 수 있습니다. 학습 데이터에 극단치(outlier)가 존재할 경우 특히 더욱 그렇습니다. 이러한 현상을 **과적합(overfitting)**이라 합니다. 이 문제를 피하기 위해 <a href=\"https://www.google.com/search?q=train+set+tvlidation+set+test+set&oq=train+set+tvlidation+set+test+set&aqs=edge..69i57j0i13.6152j0j1&sourceid=chrome&ie=UTF-8\">학습용 데이터(train set)와 검증용 데이터(validation set), 테스트 데이터(test set)를 분리할 줄 알아야 합니다.</a>\n",
        "2. 분류(classification), 군집화(clustering)\n",
        "3. 딥러닝(deep learning)\n"
      ],
      "metadata": {
        "id": "DeDEgvUXubGJ"
      }
    }
  ]
}